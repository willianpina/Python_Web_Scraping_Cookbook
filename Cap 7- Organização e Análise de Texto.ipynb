{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1 align=center>Organização e Análise de Texto</h1>\n",
    "<p align=center><img src=https://miro.medium.com/max/724/0*j9FKv2WnvVFNxU5Q.jpeg width=500></p>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Utilizando a Biblioteca NLTK</h3>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Executando a divisão de frases</h3>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Arquivo não Separado****\n",
      "We are seeking developers with demonstrable experience in: ASP.NET, C#, SQL Server, and AngularJS. We are a fast-paced, highly iterative team that has to adapt quickly as our factory grows. We need people who are comfortable tackling new problems, innovating solutions, and interacting with every facet of the company on a daily basis. Creative, motivated, able to take responsibility and support the applications you create. Help us get rockets out the door faster!\n",
      "**************************\n",
      "\n",
      "****Arquivo SEPARADO em sentenças****\n",
      "We are seeking developers with demonstrable experience in: ASP.NET, C#, SQL Server, and AngularJS.\n",
      "We are a fast-paced, highly iterative team that has to adapt quickly as our factory grows.\n",
      "We need people who are comfortable tackling new problems, innovating solutions, and interacting with every facet of the company on a daily basis.\n",
      "Creative, motivated, able to take responsibility and support the applications you create.\n",
      "Help us get rockets out the door faster!\n"
     ]
    }
   ],
   "source": [
    "# Importando separador de sentenças do NLTK\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Carregando o Arquivo\n",
    "with open('sentence1.txt','r') as myfile:\n",
    "\tdata = myfile.read().replace('\\n', '')\n",
    "print('****Arquivo não Separado****')\n",
    "print(data)\n",
    "print('**************************')\n",
    "\n",
    "# Separando a sentença\n",
    "sentences = sent_tokenize(data) # Por padrão é English, porém pode ser alterado (language=\"German\")\n",
    "print('\\n****Arquivo SEPARADO em sentenças****')\n",
    "for s in sentences:\n",
    "\tprint(s)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Execução da Tokenização</h3>\n",
    "Tokenização é o processo de conversão de texto em tokens. Esses tokens podem ser parágrafos, frases e palavras individuais comuns e geralmente são baseados no nível da palavra."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'are', 'seeking', 'developers', 'with', 'demonstrable', 'experience', 'in:', 'ASP.NET,', 'C#,', 'SQL', 'Server,', 'and', 'AngularJS.']\n"
     ]
    }
   ],
   "source": [
    "# 1. Usando o split() do Python\n",
    "first_sentence = [s for s in sentences][0]\n",
    "\n",
    "print(first_sentence.split())\n",
    "# Percebe que \":\" e \",\" são inclusos."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'are', 'seeking', 'developers', 'with', 'demonstrable', 'experience', 'in', ':', 'ASP', '.', 'NET', ',', 'C', '#,', 'SQL', 'Server', ',', 'and', 'AngularJS', '.']\n"
     ]
    }
   ],
   "source": [
    "# Usando a tokenize do NLTK\n",
    "from nltk.tokenize import word_tokenize, regexp_tokenize,wordpunct_tokenize,blankline_tokenize\n",
    "\n",
    "print(wordpunct_tokenize(first_sentence)) # Também separou a pontuação \":\" e a \",\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'are', 'seeking', 'developers', 'with', 'demonstrable', 'experience', 'in', 'ASP', 'NET', 'C', 'SQL', 'Server', 'and', 'AngularJS']\n"
     ]
    }
   ],
   "source": [
    "# Aplicando a Regex Tokenizer com uma expressão \"\\w+\"\n",
    "\n",
    "print(regexp_tokenize(first_sentence, pattern='\\w+')) # Separou melhor as palavras"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'are', 'seeking', 'developers', 'with', 'demonstrable', 'experience', 'in', ':', 'ASP.NET', ',', 'C', '#', ',', 'SQL', 'Server', ',', 'and', 'AngularJS', '.']\n"
     ]
    }
   ],
   "source": [
    "# Aplicando a wordpunckt_tokenizer\n",
    "\n",
    "print(word_tokenize(first_sentence)) # Separa a pontuação também."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are seeking developers with demonstrable experience in: ASP.NET, C#, SQL Server, and AngularJS.\n",
      "['We are seeking developers with demonstrable experience in: ASP.NET, C#, SQL Server, and AngularJS.']\n"
     ]
    }
   ],
   "source": [
    "# Aplicando a blackline_tokenize.\n",
    "print(first_sentence)\n",
    "print(blankline_tokenize(first_sentence)) # Percebe a diferença."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Executando o steamming</h3>\n",
    "Stemming é o processo de cortar um token em sua haste. Tecnicamente, é o processo ou redução de palavras flexionadas (e às vezes derivadas) ao seu radical da palavra - a forma raiz básica da palavra. Como exemplo, as palavras pescando, pescado e pescador derivam da raiz da palavra pesca. Isso ajuda a reduzir o conjunto de palavras processadas em um conjunto de base menor que é mais facilmente processado."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "'fish'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "pst = PorterStemmer()\n",
    "pst.stem('fishing')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados dos Stemmers:\n",
      "We ----- we ----- we \n",
      "are ----- are ----- ar \n",
      "seeking ----- seek ----- seek \n",
      "developers ----- develop ----- develop \n",
      "with ----- with ----- with \n",
      "demonstrable ----- demonstr ----- demonst \n",
      "experience ----- experi ----- expery \n",
      "in ----- in ----- in \n",
      "ASP ----- asp ----- asp \n",
      "NET ----- net ----- net \n",
      "C ----- c ----- c \n",
      "SQL ----- sql ----- sql \n",
      "Server ----- server ----- serv \n",
      "and ----- and ----- and \n",
      "AngularJS ----- angularj ----- angulars \n"
     ]
    }
   ],
   "source": [
    "pst = PorterStemmer()\n",
    "lst = LancasterStemmer() # Mais agressivo\n",
    "\n",
    "print('Resultados dos Stemmers:')\n",
    "for token in regexp_tokenize(sentences[0], pattern='\\w+'):\n",
    "\tprint(f\"{token} ----- {pst.stem(token)} ----- {lst.stem(token)} \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Executando a Lemmatização</h3>\n",
    "Lematização é um processo mais metódico de converter palavras em sua base. Enquanto stemming geralmente apenas corta as extremidades das palavras, a lematização leva em consideração a análise morfológica das palavras, avaliando o contexto e a parte do discurso para determinar a forma flexionada e toma uma decisão entre diferentes regras para determinar a raiz."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming / lemmatization results\n",
      "We we we We\n",
      "are are ar are\n",
      "seeking seek seek seeking\n",
      "developers develop develop developer\n",
      "with with with with\n",
      "demonstrable demonstr demonst demonstrable\n",
      "experience experi expery experience\n",
      "in in in in\n",
      "ASP asp asp ASP\n",
      "NET net net NET\n",
      "C c c C\n",
      "SQL sql sql SQL\n",
      "Server server serv Server\n",
      "and and and and\n",
      "AngularJS angularj angulars AngularJS\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "pst = PorterStemmer()\n",
    "lst = LancasterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "print(\"Stemming / lemmatization results\")\n",
    "for token in regexp_tokenize(sentences[0], pattern='\\w+'):\n",
    "    print(token, pst.stem(token), lst.stem(token), wnl.lemmatize(token))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Determinando e removendo StopWords</h3>\n",
    "Palavras de parada são palavras comuns que, em uma situação de processamento de linguagem natural, não fornecem muito significado contextual. Essas palavras são geralmente as palavras mais comuns em um idioma. Estes tendem a, pelo menos em inglês, ser artigos e pronomes, como I, me, the, is, which, who, at, entre outros."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "179"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "len(stoplist) # Tamanho da StopList (179)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "print(stoplist[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:\n",
      "We are seeking developers with demonstrable experience in: ASP.NET, C#, SQL Server, and AngularJS.\n"
     ]
    }
   ],
   "source": [
    "# Removendo as Stopwords da primeira sentença\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "with open('sentence1.txt', 'r') as myfile:\n",
    "\tdata = myfile.read().replace('\\n', '')\n",
    "\n",
    "sentences = sent_tokenize(data)\n",
    "first_sentence = sentences[0]\n",
    "print(\"Original Sentence:\")\n",
    "print(first_sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized: ['We', 'are', 'seeking', 'developers', 'with', 'demonstrable', 'experience', 'in', 'ASP', 'NET', 'C', 'SQL', 'Server', 'and', 'AngularJS']\n"
     ]
    }
   ],
   "source": [
    "tokenized = regexp_tokenize(first_sentence,'\\w+')\n",
    "print(f\"Tokenized: {tokenized}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned: ['We', 'seeking', 'developers', 'demonstrable', 'experience', 'ASP', 'NET', 'C', 'SQL', 'Server', 'AngularJS']\n"
     ]
    }
   ],
   "source": [
    "# Removendo as StopWords\n",
    "stoplist = stopwords.words(\"english\")\n",
    "cleaned = [word for word in tokenized if word not in stoplist]\n",
    "print(f\"Cleaned: {cleaned}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Calculando a distribuição de frequência das palavras</h3>\n",
    "\n",
    "Uma distribuição de frequência conta o número de ocorrências de valores de dados distintos. Eles são valiosos, pois podemos usá-los para determinar quais palavras ou frases dentro de um documento são mais comuns e, a partir disso, inferir aquelas que têm maior ou menor valor.\n",
    " As distribuições de frequência podem ser calculadas usando várias técnicas diferentes. Vamos examiná-los usando as facilidades incorporadas ao NLTK."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}